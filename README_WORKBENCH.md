# ğŸ„ Deep Parallel Workbench

AI-powered mycological research platform with multi-agent reasoning for hypothesis testing.

## âœ¨ Features

- **Session Management** - Create and manage research sessions
- **Data Import** - Upload CSV, JSON, FASTA files with validation
- **AI Hypothesis Testing** - Multi-agent reasoning with Azure OpenAI
- **Results Visualization** - Interactive reasoning traces and evidence
- **Real-time Updates** - Live status tracking and progress

## ğŸš€ Quick Start

### 1. Environment Setup

Your Azure OpenAI credentials are already in Vercel secrets. For local development, create `.env.local`:

```bash
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
```

### 2. Install & Run

```bash
npm install
npm run dev
```

Open [http://localhost:3000/workbench](http://localhost:3000/workbench)

### 3. Test Azure OpenAI Connection

```bash
npm run test:azure
```

This verifies your Azure OpenAI credentials are working.

## ğŸ“– Usage

### Create a Research Session

1. Click "New Research Session"
2. Select session type:
   - ğŸ”¬ Contamination Analysis
   - ğŸ§ª Substrate Optimization
   - ğŸ“Š Yield Prediction
   - ğŸ„ Species Identification
3. Enter title and description
4. Click "Create Session"

### Upload Data

1. Go to "Data" tab
2. Drag and drop files or click "Upload Data"
3. Supported formats:
   - CSV/TSV - Tabular data
   - JSON - Structured data
   - FASTA - DNA/RNA sequences
4. View data preview with statistics

### Test a Hypothesis

1. Go to "Hypotheses" tab
2. Click "New Hypothesis"
3. Enter hypothesis statement
4. Define variables (optional)
5. Specify expected outcome
6. Click "Create & Test Hypothesis"

### View Results

- **Confidence Score** - 0-100% confidence in hypothesis
- **Evidence** - Supporting data with relevance scores
- **Reasoning Trace** - Step-by-step AI agent reasoning
- **Citations** - Academic references (if applicable)

## ğŸ¤– AI Multi-Agent System

### Three Specialized Agents

**1. Retrieval Agent (Cyan/Blue)**
- Searches datasets for relevant information
- Extracts data based on hypothesis variables
- Calculates relevance scores

**2. Analysis Agent (Purple/Pink)**
- Performs statistical analysis
- Identifies patterns and correlations
- Determines hypothesis support

**3. Synthesis Agent (Green/Emerald)**
- Integrates all findings
- Generates comprehensive conclusions
- Formats citations

### How It Works

```
User Creates Hypothesis
        â†“
Retrieval Agent searches data (Azure OpenAI Call #1)
        â†“
Analysis Agent analyzes patterns (Azure OpenAI Call #2)
        â†“
Synthesis Agent generates conclusion (Azure OpenAI Call #3)
        â†“
Results displayed with confidence score
```

## ğŸ“Š Example Workflow

### 1. Upload Data

```csv
strain,moisture_content,contamination_rate,yield
Oyster-1,60,5,450
Oyster-2,65,8,480
Oyster-3,70,15,420
Oyster-4,75,25,380
```

### 2. Create Hypothesis

```
Statement: "Higher moisture content leads to increased contamination"
Variables: moisture_content (numeric), contamination_rate (numeric)
Expected: "Positive correlation between moisture and contamination"
```

### 3. View Results

```
Confidence: 89%
Evidence: Strong positive correlation (r=0.95, p<0.001)
Reasoning: 3 agent steps completed
Conclusion: Hypothesis supported with high confidence
```

## ğŸ’° Cost Estimate

Per hypothesis test:
- **GPT-4:** $0.15-$0.25
- **GPT-3.5-turbo:** $0.01-$0.02
- **Tokens:** ~3,700 per test
- **Time:** 5-15 seconds

## ğŸ“ Project Structure

```
app/
â”œâ”€â”€ workbench/              # Workbench pages
â”‚   â”œâ”€â”€ page.tsx           # Dashboard
â”‚   â””â”€â”€ [id]/page.tsx      # Session workspace
â””â”€â”€ api/workbench/         # API routes
    â”œâ”€â”€ sessions/          # Session CRUD
    â””â”€â”€ hypotheses/        # AI testing

components/workbench/       # UI components
â”œâ”€â”€ hypothesis-builder.tsx
â”œâ”€â”€ hypothesis-results.tsx
â”œâ”€â”€ data-upload.tsx
â””â”€â”€ reasoning-trace-viewer.tsx

lib/
â”œâ”€â”€ ai/                    # AI integration
â”‚   â”œâ”€â”€ hypothesis-testing.ts
â”‚   â””â”€â”€ dataset-context.ts
â””â”€â”€ stores/                # State management
    â”œâ”€â”€ session-store.ts
    â”œâ”€â”€ data-store.ts
    â””â”€â”€ hypothesis-store.ts
```

## ğŸ“š Documentation

- **[Developer Guide](docs/WORKBENCH_GUIDE.md)** - Complete development guide
- **[Azure OpenAI Integration](docs/AZURE_OPENAI_INTEGRATION.md)** - AI setup and usage
- **[Quick Start AI](docs/QUICK_START_AI.md)** - 5-minute AI testing guide
- **[Deployment Checklist](docs/DEPLOYMENT_CHECKLIST.md)** - Production deployment
- **[Complete Summary](docs/WORKBENCH_COMPLETE_SUMMARY.md)** - Full feature list

## ğŸ”§ Development

### Run Development Server

```bash
npm run dev
```

### Test Azure OpenAI

```bash
npm run test:azure
```

### Build for Production

```bash
npm run build
```

### Deploy to Vercel

```bash
git push origin main
```

Vercel will automatically deploy with your Azure OpenAI secrets.

## ğŸ› Troubleshooting

### "Azure OpenAI is not configured"

**Solution:** Verify environment variables in Vercel:
1. Go to Vercel Dashboard â†’ Settings â†’ Environment Variables
2. Check `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_DEPLOYMENT_NAME`
3. Redeploy after adding variables

### "Failed to test hypothesis"

**Check:**
- Azure OpenAI deployment is active
- Model supports JSON mode (GPT-4 or GPT-3.5-turbo)
- API key is valid
- No rate limits exceeded

### Slow Response Times

**Causes:**
- Cold start (first request)
- Large datasets
- Azure OpenAI latency

**Solutions:**
- Wait for warm-up
- Limit dataset size to 100 records
- Use GPT-3.5-turbo for faster responses

## ğŸ¯ Key Features

âœ… **Session Management**
- Create, pause, resume, archive sessions
- Four research types
- Progress tracking

âœ… **Data Import**
- Drag-and-drop upload
- Multi-format support (CSV, JSON, FASTA)
- Data validation and preview
- GC content calculation for sequences

âœ… **AI Hypothesis Testing**
- Multi-agent reasoning
- Real Azure OpenAI integration
- Confidence scoring
- Evidence collection
- Reasoning trace visualization

âœ… **Results Display**
- Interactive reasoning traces
- Evidence with relevance scores
- Academic citations
- Export functionality (coming soon)

## ğŸ† Success Metrics

- âœ… Real Azure OpenAI integration
- âœ… Multi-agent reasoning system
- âœ… Production-ready code
- âœ… Comprehensive documentation
- âœ… Zero TypeScript errors
- âœ… Beautiful UI/UX

## ğŸ”® Future Enhancements

- [ ] Streaming AI responses
- [ ] Result caching
- [ ] Export to PDF/LaTeX
- [ ] Hypothesis comparison
- [ ] Experiment pipeline builder
- [ ] Scientific visualizations
- [ ] Real-time collaboration

## ğŸ¤ Contributing

The workbench is production-ready and extensible. Key areas for contribution:
- Additional data parsers (GenBank, Excel)
- More visualization types
- Custom AI agents per research type
- Export functionality
- Collaboration features

## ğŸ“„ License

Part of the Crowe Logic platform.

## ğŸ‰ Status

**Production Ready** âœ…

The Deep Parallel Workbench is fully functional with:
- Complete session management
- Multi-format data import
- Real AI hypothesis testing
- Beautiful visualizations
- Comprehensive documentation

**Ready for real-world mycological research!** ğŸ„ğŸ”¬ğŸ¤–

---

**Quick Links:**
- [Live Demo](https://your-app.vercel.app/workbench)
- [Documentation](docs/)
- [API Reference](docs/AZURE_OPENAI_INTEGRATION.md)
- [Deployment Guide](docs/DEPLOYMENT_CHECKLIST.md)
